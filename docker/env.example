# BeigeBox local overrides — copy to .env and edit for your machine
# cp env.example .env
#
# QUICK REFERENCE:
#   docker compose up -d                         # full stack (CPU inference)
#   docker compose --profile voice up -d         # full stack + voice I/O
#   docker compose up -d beigebox                # proxy only, external Ollama

# ── Ports (host-side) ────────────────────────────────────────────────────────
BEIGEBOX_PORT=1337
OLLAMA_PORT=11434
WEBUI_PORT=3000
WHISPER_PORT=9000
KOKORO_PORT=8880

# ── Ollama ───────────────────────────────────────────────────────────────────
# Mount your host ~/.ollama so the container shares your existing models
# avoids re-downloading everything
OLLAMA_DATA=/home/youruser/.ollama

# Which Ollama service to route to — matches the service name in compose
# Leave as 'ollama' for Docker stack, or set a LAN IP for remote e.g. 192.168.1.x
OLLAMA_HOST=ollama

# ── API keys (optional) ──────────────────────────────────────────────────────
# Only needed if using Google Search tool instead of DuckDuckGo
GOOGLE_API_KEY=
GOOGLE_CSE_ID=

# ── Open WebUI ───────────────────────────────────────────────────────────────
# false = no login required (local use), true = enable auth
WEBUI_AUTH=false

# ── Voice (voice profile only) ───────────────────────────────────────────────
# Whisper model — larger = more accurate, slower to load
# Options: Systran/faster-whisper-tiny, base, small, medium, large-v3
WHISPER_MODEL=Systran/faster-whisper-base
