# BeigeBox local overrides — copy to .env and edit for your machine
# cp .env.example .env
#
# PROFILE QUICK REFERENCE:
#   docker compose --profile gpu up -d                   # NVIDIA GPU stack
#   docker compose --profile cpu up -d                   # CPU-only stack
#   docker compose --profile gpu --profile voice up -d   # GPU + STT/TTS
#   docker compose up -d beigebox                        # proxy only, external Ollama

# ── Ports (host-side) ────────────────────────────────────────────────────────
BEIGEBOX_PORT=1337
OLLAMA_PORT=11434
WEBUI_PORT=3000
WHISPER_PORT=9000
KOKORO_PORT=8880

# ── Ollama ───────────────────────────────────────────────────────────────────
# Mount your host ~/.ollama so the container shares your existing models
# avoids re-downloading everything
OLLAMA_DATA=/home/youruser/.ollama

# Which Ollama container to route to — matches the service name in compose
# use 'ollama' for gpu profile, 'ollama-cpu' for cpu profile
# use a LAN IP to point at a remote machine e.g. http://192.168.1.x:11434
OLLAMA_HOST=ollama

# ── API keys (optional) ──────────────────────────────────────────────────────
# Only needed if using Google Search tool instead of DuckDuckGo
GOOGLE_API_KEY=
GOOGLE_CSE_ID=

# ── Open WebUI ───────────────────────────────────────────────────────────────
# false = no login required (local use), true = enable auth
WEBUI_AUTH=false

# ── Voice (voice profile only) ───────────────────────────────────────────────
# Whisper model — larger = more accurate, slower to load
# Options: Systran/faster-whisper-tiny, base, small, medium, large-v3
WHISPER_MODEL=Systran/faster-whisper-base
