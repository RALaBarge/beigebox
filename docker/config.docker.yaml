# beigebox config — Docker deployment
# Uses container service names for networking.
# For bare-metal, use config.yaml instead.

# --- Backend (where requests get forwarded) ---
backend:
  url: "http://ollama:11434"            # Docker service name
  default_model: "llama3.2:3b"         # Main chat model — pulled by ollama-init
  timeout: 120

# --- Middleware Server ---
server:
  host: "0.0.0.0"
  port: 8000

# --- Embedding ---
embedding:
  model: "nomic-embed-text"
  backend_url: "http://ollama:11434"

# --- Storage ---
storage:
  sqlite_path: "./data/conversations.db"
  chroma_path: "./data/chroma"
  log_conversations: true

# --- Tools ---
tools:
  enabled: true
  webhook_url: ""
  web_search:
    enabled: true
    provider: "duckduckgo"
    max_results: 5
  web_scraper:
    enabled: true
    max_content_length: 10000
  google_search:
    enabled: false
    api_key: "${GOOGLE_API_KEY}"
    cse_id: "${GOOGLE_CSE_ID}"
  calculator:
    enabled: true
  datetime:
    enabled: true
    local_tz_offset: -5.0
  system_info:
    enabled: true
  memory:
    enabled: true
    max_results: 3
    min_score: 0.3
  ensemble:
    enabled: false
    judge_model: ""
    max_models: 6

# --- Decision LLM ---
decision_llm:
  enabled: true                          # Enabled by default in Docker — setup pulls the model
  model: "llama3.2:3b"                   # Fast routing decisions
  backend_url: "http://ollama:11434"
  timeout: 5
  max_tokens: 256
  routes:
    default:
      model: "llama3.2:3b"
      description: "General purpose conversation and Q&A"
    code:
      model: "llama3.2:3b"
      description: "Code generation, debugging, technical programming tasks"
    large:
      model: "llama3.2:3b"
      description: "Complex reasoning, analysis, long-form writing"
    fast:
      model: "llama3.2:3b"
      description: "Quick responses, simple questions, brainstorming"

# --- Operator ---
operator:
  enabled: false                          # CAUTION: enables LLM-driven tool execution. Review allowed_tools before enabling.
  model: "llama3.2:3b"                   # Defaults to backend.default_model
  max_iterations: 10
  allowed_tools: []                       # Empty = all registered tools. Restrict: ["system_info", "memory", "calculator", "datetime"]
  shell:
    enabled: true
    # All shell commands are routed through /usr/local/bin/bb (restricted busybox).
    # This allowlist is the first gate; the bb wrapper enforces a blocklist as the second.
    # Only add applets that are genuinely useful for system observability.
    shell_binary: "/usr/local/bin/bb"
    allowed_commands:
      - ls
      - cat
      - grep
      - ps
      - df
      - free
      - du
      - uptime
      - uname
      - env
      - printenv
      - whoami
      - id
      - hostname
      - date
    blocked_patterns:
      # Belt-and-suspenders: reject at the config layer even if the applet
      # somehow passes the bb wrapper check.
      - "rm"
      - "chmod"
      - "chown"
      - "chroot"
      - "mv"
      - "cp"
      - "> "          # output redirection
      - ">>"          # append redirection
      - "sudo"
      - "su "
      - "| sh"        # pipe to shell
      - "| bash"
      - "| ash"
      - "$()"         # command substitution
      - "`"           # backtick substitution
  data:
    sqlite_queries:   # named query templates the agent can call by name
      recent_conversations: >
        SELECT c.id, c.created_at, COUNT(m.id) as msg_count
        FROM conversations c JOIN messages m ON m.conversation_id = c.id
        GROUP BY c.id ORDER BY c.created_at DESC LIMIT 10
      token_usage_by_model: >
        SELECT model, SUM(token_count) as tokens, COUNT(*) as messages
        FROM messages GROUP BY model ORDER BY tokens DESC
      search_messages: >
        SELECT role, content, model, timestamp FROM messages
        WHERE content LIKE ? ORDER BY timestamp DESC LIMIT 20
      todays_conversations: >
        SELECT c.id, c.created_at, COUNT(m.id) as msg_count
        FROM conversations c JOIN messages m ON m.conversation_id = c.id
        WHERE date(c.created_at) = date('now')
        GROUP BY c.id ORDER BY c.created_at DESC

# --- Hooks ---
hooks:
  directory: "./hooks"

# --- Logging ---
logging:
  level: "INFO"
  file: "./data/beigebox.log"

# --- Wiretap ---
wiretap:
  path: "./data/wire.jsonl"

# --- Auto-summarization ---
auto_summarization:
  enabled: false               # Enable to compress long conversations automatically
  token_budget: 3000           # Trigger when estimated tokens exceed this
  summary_model: "llama3.2:3b" # Model to use for summarization (defaults to backend.default_model)
  keep_last: 4                 # Always keep the N most recent messages intact
  summary_prefix: "Summary of earlier conversation: "

# --- System Context (global prompt injection) ---
system_context:
  enabled: false
  path: ./system_context.md

# --- Generation Parameters (global inference overrides) ---
# Uncomment values to override. Leave unset to let frontend decide.
# generation:
#   force: false
#   temperature: ~
#   top_p: ~
#   top_k: ~
#   num_ctx: ~
#   repeat_penalty: ~
#   max_tokens: ~
#   seed: ~

# --- Voice (Push-to-talk + TTS autoplay) ---
voice:
  enabled: false               # Enable voice input/output
  stt_url: "http://whisper:8000"   # faster-whisper-server container
  tts_url: "http://kokoro:8880"    # kokoro-fastapi container
  tts_voice: "af_heart"            # Kokoro default — see /v1/audio/voices for full list
  tts_model: "kokoro"
  tts_speed: 1.0
  tts_autoplay: false          # Set true to auto-speak assistant responses
  hotkey: ""                   # Push-to-talk hotkey (e.g. "v")
