# BeigeBox — Full Stack
# Tap the line. Control the carrier.
#
# QUICKSTART:
#   ./setup.sh              # pulls models, starts everything
#
# OR MANUALLY:
#   docker compose up -d                     # full stack (Ollama + BeigeBox + Open WebUI)
#   docker compose up -d beigebox open-webui # already have Ollama on host
#
# SERVICES:
#   ollama     → :11434  LLM inference engine (GPU)
#   beigebox   → :1337   transparent proxy + middleware
#   open-webui → :3000   chat interface
#
# DATA:
#   ollama_data    → host ~/.ollama mounted directly (shares host models)
#   beigebox_data  → conversations, embeddings, logs (persistent)
#   open_webui_data → Open WebUI settings/users (persistent)

services:
  # ---------------------------------------------------------------------------
  # Ollama — LLM inference
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: beigebox-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ${OLLAMA_DATA:-/home/jinx/.ollama}:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s #load gpu drivers
  ollama-model-pull:
    image: curlimages/curl
    depends_on: [ollama]
    command: >
      sh -c "sleep 3 &&
      curl -s http://ollama:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}' &&
      curl -s http://ollama:11434/api/pull -d '{\"name\":\"llama3.2:3b\"}'"
    restart: "no"

  # ---------------------------------------------------------------------------
  # BeigeBox — transparent LLM proxy + middleware
  # ---------------------------------------------------------------------------
  beigebox:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: beigebox
    ports:
      - "${BEIGEBOX_PORT:-1337}:8000"
    volumes:
      - beigebox_data:/app/data
      - ../hooks:/app/hooks:ro
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GOOGLE_CSE_ID=${GOOGLE_CSE_ID:-}
      - OLLAMA_HOST=ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/beigebox/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # Open WebUI — chat frontend
  # ---------------------------------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: beigebox-webui
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      # Route ALL chat traffic through BeigeBox
      - OPENAI_API_BASE_URL=http://beigebox:8000/v1
      - OPENAI_API_KEY=not-needed
      # Direct Ollama connection for model management (pull/delete/etc)
      - OLLAMA_BASE_URL=http://ollama:11434
      # Disable Open WebUI's built-in auth for local use (optional)
      - WEBUI_AUTH=${WEBUI_AUTH:-false}
    depends_on:
      beigebox:
        condition: service_started
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # faster-whisper-server — STT (OpenAI-compatible /v1/audio/transcriptions)
  # ---------------------------------------------------------------------------
  whisper:
    image: fedirz/faster-whisper-server:latest-cpu   # swap to :latest-cuda for GPU
    container_name: beigebox-whisper
    ports:
      - "${WHISPER_PORT:-9000}:8000"
    volumes:
      - whisper_cache:/root/.cache/huggingface
    environment:
      - WHISPER__MODEL=${WHISPER_MODEL:-Systran/faster-whisper-base}
      # For GPU: set WHISPER__DEVICE=cuda and use the cuda image tag
      - WHISPER__DEVICE=cpu
      - WHISPER__INFERENCE_DEVICE=cpu
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s   # model download on first start

  # ---------------------------------------------------------------------------
  # Kokoro-FastAPI — TTS (OpenAI-compatible /v1/audio/speech)
  # ---------------------------------------------------------------------------
  kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest   # swap to kokoro-fastapi-gpu:latest for GPU
    container_name: beigebox-kokoro
    ports:
      - "${KOKORO_PORT:-8880}:8880"
    volumes:
      - kokoro_models:/app/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # model download on first start

volumes:
  beigebox_data:
  open_webui_data:
  whisper_cache:
  kokoro_models:
