# BeigeBox — Full Stack
# Tap the line. Control the carrier.
#
# PROFILES:
#   (none)          beigebox only — point OLLAMA_URL in .env at existing Ollama
#   gpu             beigebox + Ollama (NVIDIA GPU) + Open WebUI
#   cpu             beigebox + Ollama (CPU only)   + Open WebUI
#   voice           adds Whisper (STT) + Kokoro (TTS) — combine with gpu or cpu
#
# EXAMPLES:
#   docker compose --profile gpu up -d                    # local GPU stack
#   docker compose --profile cpu up -d                    # CPU-only stack
#   docker compose --profile gpu --profile voice up -d    # GPU + voice
#   docker compose up -d beigebox                         # proxy only, external Ollama
#
# CONFIGURATION:
#   cp .env.example .env  → set OLLAMA_DATA, ports, API keys
#
# SERVICES:
#   ollama      → :11434  LLM inference (gpu profile)
#   ollama-cpu  → :11434  LLM inference (cpu profile)
#   beigebox    → :1337   transparent proxy + middleware (always)
#   open-webui  → :3000   chat interface (gpu or cpu profile)
#   whisper     → :9000   speech-to-text (voice profile)
#   kokoro      → :8880   text-to-speech (voice profile)

services:
  # ---------------------------------------------------------------------------
  # Ollama — GPU (NVIDIA)
  # ---------------------------------------------------------------------------
  ollama:
    profiles: [gpu]
    image: ollama/ollama:latest
    container_name: beigebox-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ${OLLAMA_DATA:-~/.ollama}:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ---------------------------------------------------------------------------
  # Ollama — CPU only
  # ---------------------------------------------------------------------------
  ollama-cpu:
    profiles: [cpu]
    image: ollama/ollama:latest
    container_name: beigebox-ollama-cpu
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ${OLLAMA_DATA:-~/.ollama}:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ---------------------------------------------------------------------------
  # Model pull — runs once on first start to ensure required models exist
  # ---------------------------------------------------------------------------
  ollama-model-pull:
    profiles: [gpu, cpu]
    image: curlimages/curl
    container_name: beigebox-model-pull
    command: >
      sh -c "sleep 3 &&
      curl -s http://${OLLAMA_HOST:-ollama}:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}' &&
      curl -s http://${OLLAMA_HOST:-ollama}:11434/api/pull -d '{\"name\":\"llama3.2:3b\"}'"
    restart: "no"

  # ---------------------------------------------------------------------------
  # BeigeBox — transparent LLM proxy + middleware (always on)
  # ---------------------------------------------------------------------------
  beigebox:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: beigebox
    ports:
      - "${BEIGEBOX_PORT:-1337}:8000"
    volumes:
      - beigebox_data:/app/data
      - ../hooks:/app/hooks:ro
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GOOGLE_CSE_ID=${GOOGLE_CSE_ID:-}
      - OLLAMA_HOST=${OLLAMA_HOST:-ollama}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/beigebox/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # Open WebUI — chat frontend
  # ---------------------------------------------------------------------------
  open-webui:
    profiles: [gpu, cpu]
    image: ghcr.io/open-webui/open-webui:main
    container_name: beigebox-webui
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://beigebox:8000/v1
      - OPENAI_API_KEY=not-needed
      - OLLAMA_BASE_URL=http://${OLLAMA_HOST:-ollama}:11434
      - WEBUI_AUTH=${WEBUI_AUTH:-false}
    depends_on:
      beigebox:
        condition: service_started
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Whisper — STT (voice profile)
  # ---------------------------------------------------------------------------
  whisper:
    profiles: [voice]
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: beigebox-whisper
    ports:
      - "${WHISPER_PORT:-9000}:8000"
    volumes:
      - whisper_cache:/root/.cache/huggingface
    environment:
      - WHISPER__MODEL=${WHISPER_MODEL:-Systran/faster-whisper-base}
      - WHISPER__DEVICE=cpu
      - WHISPER__INFERENCE_DEVICE=cpu
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Kokoro — TTS (voice profile)
  # ---------------------------------------------------------------------------
  kokoro:
    profiles: [voice]
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest
    container_name: beigebox-kokoro
    ports:
      - "${KOKORO_PORT:-8880}:8880"
    volumes:
      - kokoro_models:/app/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

volumes:
  beigebox_data:
  open_webui_data:
  whisper_cache:
  kokoro_models:
