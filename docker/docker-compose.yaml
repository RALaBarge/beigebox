# BeigeBox — Full Stack
# Tap the line. Control the carrier.
#
# QUICKSTART:
#   docker compose up -d          ← full working stack (CPU inference)
#
# PROFILES:
#   (default)       Ollama (CPU) + BeigeBox + Open WebUI — everything you need
#   voice           adds Whisper (STT) + Kokoro (TTS)
#
# EXAMPLES:
#   docker compose up -d                         # full stack, CPU inference
#   docker compose --profile voice up -d         # full stack + voice I/O
#   docker compose up -d beigebox                # proxy only (external Ollama)
#
# NVIDIA GPU:
#   Uncomment the 'deploy' block on the ollama service below, then:
#   docker compose up -d
#
# CONFIGURATION:
#   cp env.example .env  → set OLLAMA_DATA, ports, API keys
#
# SERVICES:
#   ollama      → :11434  LLM inference engine
#   beigebox    → :1337   transparent proxy + middleware
#   open-webui  → :3000   chat interface
#   whisper     → :9000   speech-to-text (voice profile)
#   kokoro      → :8880   text-to-speech (voice profile)

services:
  # ---------------------------------------------------------------------------
  # Ollama — LLM inference (CPU by default)
  # ---------------------------------------------------------------------------
  # For NVIDIA GPU: uncomment the 'deploy' block below and restart.
  ollama:
    image: ollama/ollama:latest
    container_name: beigebox-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ${OLLAMA_DATA:-ollama_data}:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # ── Uncomment below for NVIDIA GPU acceleration ──
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # ---------------------------------------------------------------------------
  # Model pull — runs once to ensure required models exist
  # ---------------------------------------------------------------------------
  ollama-model-pull:
    image: curlimages/curl
    container_name: beigebox-model-pull
    depends_on:
      ollama:
        condition: service_started
    command: >
      sh -c "sleep 5 &&
      curl -s http://ollama:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}' &&
      curl -s http://ollama:11434/api/pull -d '{\"name\":\"llama3.2:3b\"}'"
    restart: "no"

  # ---------------------------------------------------------------------------
  # BeigeBox — transparent LLM proxy + middleware
  # ---------------------------------------------------------------------------
  beigebox:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: beigebox
    ports:
      - "${BEIGEBOX_PORT:-1337}:8000"
    volumes:
      - beigebox_data:/app/data
      - ../hooks:/app/hooks:ro
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GOOGLE_CSE_ID=${GOOGLE_CSE_ID:-}
      - OLLAMA_HOST=${OLLAMA_HOST:-ollama}
    depends_on:
      ollama:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/beigebox/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # Open WebUI — chat frontend
  # ---------------------------------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: beigebox-webui
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://beigebox:8000/v1
      - OPENAI_API_KEY=not-needed
      - OLLAMA_BASE_URL=http://${OLLAMA_HOST:-ollama}:11434
      - WEBUI_AUTH=${WEBUI_AUTH:-false}
    depends_on:
      beigebox:
        condition: service_started
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Whisper — STT (voice profile)
  # ---------------------------------------------------------------------------
  whisper:
    profiles: [voice]
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: beigebox-whisper
    ports:
      - "${WHISPER_PORT:-9000}:8000"
    volumes:
      - whisper_cache:/root/.cache/huggingface
    environment:
      - WHISPER__MODEL=${WHISPER_MODEL:-Systran/faster-whisper-base}
      - WHISPER__DEVICE=cpu
      - WHISPER__INFERENCE_DEVICE=cpu
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Kokoro — TTS (voice profile)
  # ---------------------------------------------------------------------------
  kokoro:
    profiles: [voice]
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest
    container_name: beigebox-kokoro
    ports:
      - "${KOKORO_PORT:-8880}:8880"
    volumes:
      - kokoro_models:/app/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

volumes:
  ollama_data:
  beigebox_data:
  open_webui_data:
  whisper_cache:
  kokoro_models:
