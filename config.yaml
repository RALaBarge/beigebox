# beigebox config
# All runtime settings live here. No hardcoded values in the codebase.

# --- Backend (where requests get forwarded) ---
backend:
  url: "http://localhost:11434"       # Ollama base URL
  default_model: ""                   # Fallback model if none specified in request (set to a model you have pulled)
  timeout: 120                         # Seconds before we give up on a backend response

# --- Middleware Server ---
server:
  host: "0.0.0.0"
  port: 8001

# --- Embedding ---
embedding:
  model: "nomic-embed-text"            # Must be pulled in Ollama
  backend_url: "http://localhost:11434" # Can point to a different Ollama instance

# --- Storage ---
storage:
  sqlite_path: "./data/conversations.db"
  chroma_path: "./data/chroma"
  log_conversations: true              # Master switch: set false to disable all capture

# --- Tools ---
tools:
  enabled: true                        # Master switch for LangChain tool augmentation
  webhook_url: ""                      # Send tool invocations here (e.g. "tcp://localhost:9999" for nc -lk 9999)
  web_search:
    enabled: true
    provider: "duckduckgo"             # "duckduckgo" or "google"
    max_results: 5
  web_scraper:
    enabled: true
    max_content_length: 10000          # Max chars to extract per page
  google_search:
    enabled: false
    api_key: "${GOOGLE_API_KEY}"       # Resolved from .env at runtime
    cse_id: "${GOOGLE_CSE_ID}"
  calculator:
    enabled: true                      # Safe math expression evaluator
  datetime:
    enabled: true                      # Current time/date, timezone conversions
    local_tz_offset: -5.0              # Your timezone offset from UTC (EST = -5)
  system_info:
    enabled: true                      # Host system stats (CPU, RAM, GPU, disk)
  memory:
    enabled: true                      # Semantic search over past conversations
    max_results: 3
    min_score: 0.3                     # Minimum similarity (0-1) to include

# --- Decision LLM ---
# A small, fast model that routes requests to the right backend model
# and decides if tools/RAG are needed. ~3GB RAM for the MoE variant.
# Set enabled: true and pull the model to activate.
decision_llm:
  enabled: false                       # Set to true after pulling the model
  model: ""                           # A small, fast model for routing (e.g. MoE variant with low active params)
  backend_url: "http://localhost:11434"
  timeout: 5                           # Seconds — if router is slow, skip and forward raw
  max_tokens: 256                      # Routing decisions should be tiny

  # Routes: named shortcuts to models. The decision LLM picks a route name,
  # BeigeBox resolves it to the actual model string.
  # Add/remove routes as you pull/remove models from Ollama.
  routes:
    default:
      model: ""                        # Set to your general-purpose model
      description: "General purpose conversation and Q&A"
    code:
      model: ""                        # Set to your code-specialized model
      description: "Code generation, debugging, technical programming tasks"
    large:
      model: ""                        # Set to your largest/smartest model
      description: "Complex reasoning, analysis, long-form writing"
    fast:
      model: ""                        # Set to your fastest model (e.g. small or MoE)
      description: "Quick responses, simple questions, brainstorming"

# --- Hooks ---
# Extensible pre/post processing pipeline.
# Drop .py files in the hooks directory, or specify paths explicitly.
hooks:
  directory: "./hooks"                 # Auto-load all .py files from this directory
  # Or specify individual hooks with explicit paths:
  # hooks:
  #   - name: "my_hook"
  #     path: "./hooks/my_hook.py"
  #     enabled: true

# --- Logging ---
logging:
  level: "INFO"                        # DEBUG, INFO, WARNING, ERROR
  file: "./data/beigebox.log"

# --- Wiretap (structured conversation log for `beigebox tap`) ---
wiretap:
  path: "./data/wire.jsonl"            # JSONL log of every message on the wire


###---- Move this to where it needs to be when finished:

operator:
  model: ""              # which Ollama model to use (defaults to backend.default_model)
  max_iterations: 10
  shell:
    enabled: true
    allowed_commands:    # ONLY these base commands can be called — you edit this list
      - ls
      - cat
      - grep
      - ps
      - df
      - free
      - ollama
      - beigebox
    blocked_patterns:    # extra safety — reject even allowlisted commands if they match
      - "rm -rf"
      - "sudo"
      - "> /etc"
  data:
    sqlite_queries:      # named query templates the agent can call by name
      recent_conversations: >
        SELECT c.id, c.created_at, COUNT(m.id) as msg_count
        FROM conversations c JOIN messages m ON m.conversation_id = c.id
        GROUP BY c.id ORDER BY c.created_at DESC LIMIT 10
      token_usage_by_model: >
        SELECT model, SUM(token_count) as tokens, COUNT(*) as messages
        FROM messages GROUP BY model ORDER BY tokens DESC
      search_messages: >
        SELECT role, content, model, timestamp FROM messages
        WHERE content LIKE ? ORDER BY timestamp DESC LIMIT 20
      todays_conversations: >
        SELECT c.id, c.created_at, COUNT(m.id) as msg_count
        FROM conversations c JOIN messages m ON m.conversation_id = c.id
        WHERE date(c.created_at) = date('now')
        GROUP BY c.id ORDER BY c.created_at DESC
