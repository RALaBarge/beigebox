# beigebox config
# All runtime settings live here. No hardcoded values in the codebase.

# --- Backend (where requests get forwarded) ---
backend:
  url: "http://192.168.1.214:11434"       # Ollama base URL
  default_model: "gpt-oss-20b-GGUF:Q4_K_M"                   # Fallback model if none specified in request (set to a model you have pulled)
  timeout: 120                         # Seconds before we give up on a backend response

# --- Middleware Server ---
server:
  host: "0.0.0.0"
  port: 8001

# --- Embedding ---
embedding:
  model: "nomic-embed-text"            # Must be pulled in Ollama
  backend_url: "http://localhost:11434" # Can point to a different Ollama instance

# --- Storage ---
storage:
  sqlite_path: "./data/conversations.db"
  chroma_path: "./data/chroma"
  log_conversations: true              # Master switch: set false to disable all capture

# --- Tools ---
tools:
  enabled: true                        # Master switch for LangChain tool augmentation
  webhook_url: ""                      # Send tool invocations here (e.g. "tcp://localhost:9999" for nc -lk 9999)
  web_search:
    enabled: true
    provider: "duckduckgo"             # "duckduckgo" or "google"
    max_results: 5
  web_scraper:
    enabled: true
    max_content_length: 10000          # Max chars to extract per page
  google_search:
    enabled: false
    api_key: "${GOOGLE_API_KEY}"       # Resolved from .env at runtime
    cse_id: "${GOOGLE_CSE_ID}"
  calculator:
    enabled: true                      # Safe math expression evaluator
  datetime:
    enabled: true                      # Current time/date, timezone conversions
    local_tz_offset: -5.0              # Your timezone offset from UTC (EST = -5)
  system_info:
    enabled: true                      # Host system stats (CPU, RAM, GPU, disk)
  memory:
    enabled: true                      # Semantic search over past conversations
    max_results: 3
    min_score: 0.3                     # Minimum similarity (0-1) to include

# --- Decision LLM ---
# A small, fast model that routes requests to the right backend model
# and decides if tools/RAG are needed. ~3GB RAM for the MoE variant.
# Set enabled: true and pull the model to activate.
decision_llm:
  enabled: false                       # Set to true after pulling the model
  model: "llama3.2:3b"                           # A small, fast model for routing (e.g. MoE variant with low active params)
  backend_url: "http://localhost:11434"
  timeout: 5                           # Seconds â€” if router is slow, skip and forward raw
  max_tokens: 256                      # Routing decisions should be tiny

  # Routes: named shortcuts to models. The decision LLM picks a route name,
  # BeigeBox resolves it to the actual model string.
  # Add/remove routes as you pull/remove models from Ollama.
  routes:
    default:
      model: ""                        # Set to your general-purpose model
      description: "General purpose conversation and Q&A"
    code:
      model: ""                        # Set to your code-specialized model
      description: "Code generation, debugging, technical programming tasks"
    large:
      model: ""                        # Set to your largest/smartest model
      description: "Complex reasoning, analysis, long-form writing"
    fast:
      model: ""                        # Set to your fastest model (e.g. small or MoE)
      description: "Quick responses, simple questions, brainstorming"

# --- Hooks ---
# Extensible pre/post processing pipeline.
# Drop .py files in the hooks directory, or specify paths explicitly.
hooks:
  directory: "./hooks"                 # Auto-load all .py files from this directory
  # Or specify individual hooks with explicit paths:
  # hooks:
  #   - name: "my_hook"
  #     path: "./hooks/my_hook.py"
  #     enabled: true

# --- Logging ---
logging:
  level: "INFO"                        # DEBUG, INFO, WARNING, ERROR
  file: "./data/beigebox.log"

# --- Wiretap (structured conversation log for `beigebox tap`) ---
wiretap:
  path: "./data/wire.jsonl"            # JSONL log of every message on the wire

routing:
  session_ttl_seconds: 1800

# --- Model Advertising ---
# Control whether BeigeBox advertises its presence in the model list.
# This affects what Open WebUI shows in the model dropdown.
model_advertising:
  mode: "hidden"                   # "advertise" or "hidden"
  # hidden:    Shows: [llama3.2, gpt-oss-20b, claude]
  #            (BeigeBox is transparent, user doesn't know it's there)
  # advertise: Shows: [beigebox:llama3.2, beigebox:gpt-oss-20b, beigebox:claude]
  #            (User knows they're talking through the middleware)
  
  prefix: "beigebox:"              # Prefix to add in advertise mode
  #                                # Examples: "ðŸ”— ", "[middleware] ", "beigebox:" etc.

# --- Multi-Backend Support (v0.6.0) ---
# Support for multiple backends with priority-based fallback
# Currently supports: Ollama (local), OpenRouter (API)
backends:
  - name: "local"
    url: "http://localhost:11434"
    provider: "ollama"
    priority: 1
    timeout: 120
    
  - name: "openrouter"
    url: "https://openrouter.ai/api/v1"
    provider: "openrouter"
    api_key: "${OPENROUTER_API_KEY}"
    priority: 2
    timeout: 60
    # Note: API key from environment, not config file (security)

backends_enabled: false

# --- Cost Tracking (v0.6.0) ---
# Track costs for OpenRouter API calls (local models are $0)
# OpenRouter returns cost_usd with each response
cost_tracking:
  enabled: false
  track_openrouter: true
  track_local: false

# --- Orchestrator (v0.6.0) ---
# Allow Operator agent to spawn parallel LLM tasks
# Useful for divide-and-conquer on complex problems
orchestrator:
  enabled: false
  max_parallel_tasks: 5
  task_timeout_seconds: 120
  total_timeout_seconds: 300

# --- Flight Recorder (v0.6.0) ---
# Record detailed timeline of each request's lifecycle
# Helps debug routing decisions and performance bottlenecks
flight_recorder:
  enabled: false
  retention_hours: 24
  max_records: 1000

# --- Conversation Replay (v0.6.0) ---
# Reconstruct full conversations with routing decisions
# Shows which model, why, what tools, confidence scores
conversation_replay:
  enabled: false

# --- Semantic Conversation Map (v0.6.0) ---
# Visualize conversation topics as a semantic graph
# Shows topic clustering and discussion flow
semantic_map:
  enabled: false
  similarity_threshold: 0.5
  max_topics: 50

# --- Reserved for v0.7.0+ ---
# Add features below as they're implemented

# prompt_injection_detection:
#   enabled: false

# fine_tuning_export:
#   enabled: false

# voice_pipeline:
#   enabled: false

# model_performance_dashboard:
#   enabled: false

# tap_filters:
#   enabled: false

###---- Move this to where it needs to be when finished:


operator:
  model: "llama3.2:3b"              # which Ollama model to use (defaults to backend.default_model)
  max_iterations: 10
  shell:
    enabled: true
    allowed_commands:    # ONLY these base commands can be called â€” you edit this list
      - ls
      - cat
      - grep
      - ps
      - df
      - free
      - ollama
      - beigebox
    blocked_patterns:    # extra safety â€” reject even allowlisted commands if they match
      - "rm -rf"
      - "sudo"
      - "> /etc"
  data:
    sqlite_queries:      # named query templates the agent can call by name
      recent_conversations: >
        SELECT c.id, c.created_at, COUNT(m.id) as msg_count
        FROM conversations c JOIN messages m ON m.conversation_id = c.id
        GROUP BY c.id ORDER BY c.created_at DESC LIMIT 10
      token_usage_by_model: >
        SELECT model, SUM(token_count) as tokens, COUNT(*) as messages
        FROM messages GROUP BY model ORDER BY tokens DESC
      search_messages: >
        SELECT role, content, model, timestamp FROM messages
        WHERE content LIKE ? ORDER BY timestamp DESC LIMIT 20
      todays_conversations: >
        SELECT c.id, c.created_at, COUNT(m.id) as msg_count
        FROM conversations c JOIN messages m ON m.conversation_id = c.id
        WHERE date(c.created_at) = date('now')
        GROUP BY c.id ORDER BY c.created_at DESC
